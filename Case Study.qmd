---
title: "Case_Study"
format: html
editor: visual
---

## 1. Setup and Data Loading

First, we load all the necessary R packages for data manipulation, modeling, and visualization. We also set some global options for the code chunks.

```{r setup, message=FALSE, warning=FALSE}
# Load necessary libraries
library(tidyverse)    # For data manipulation and visualization
library(moments)      # For moment calculations
library(car)          # For VIF calculation in linear models
library(mgcv)         # For Generalized Additive Models (GAMs)
library(randomForest) # For Random Forest models
library(kernlab)      # For Gaussian Process models
library(knitr)        # For creating well-formatted tables
library(ggplot2)
library(dplyr)
library(tidyr)
library(tibble)
library(readr)
library(ranger)
library(purrr)
library(patchwork)
library(gratia)

# Set global options for code chunks
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE, fig.width = 10, fig.height = 6)
```

### 1.1. Load Raw Data

We load the training and test datasets provided for the case study. \[1, 1\]

```{r load-data}
# Load the training and test data
train_raw <- read.csv("data-train.csv")
test_raw <- read.csv("data-test.csv")

# Display the first few rows of the training data
head(train_raw)
```

## 2. Data Preparation and Feature Engineering

This section covers the transformation of the raw simulation output into a format suitable for modeling.

### 2.1. Moment Conversion

The raw data provides the first four raw moments of the particle cluster volume distribution. We need to convert these into the more interpretable summary statistics: mean, standard deviation, skewness, and kurtosis. We create a function to handle this conversion.

```{r moment-conversion}
## ---- moment-conversion-fixed, message=FALSE, warning=FALSE -------------------

# Safer conversion of raw moments → mean, sd, skewness, kurtosis
calculate_summary_stats <- function(raw_df, eps = 1e-8, return_flags = FALSE) {
  
  # Check for required columns
  required_cols <- c("R_moment_1", "R_moment_2", "R_moment_3", "R_moment_4")
  if (!all(required_cols %in% names(raw_df))) {
    stop("Input must contain columns: R_moment_1–R_moment_4")
  }
  
  n <- nrow(raw_df)
  
  # Build raw moment matrix (rows = moment order)
  raw_mat <- rbind(
    rep(1, n),              # 0th raw moment
    raw_df$R_moment_1,      # 1st
    raw_df$R_moment_2,      # 2nd
    raw_df$R_moment_3,      # 3rd
    raw_df$R_moment_4       # 4th
  )
  
  # Convert to central moments
  central_mat <- raw2central(raw_mat)
  
  # Extract central moments
  mu   <- raw_df$R_moment_1
  mu2  <- central_mat[3, ]  # variance
  mu3  <- central_mat[4, ]
  mu4  <- central_mat[5, ]
  
  # Clean numerical artifacts
  mu2_clean <- pmax(mu2, 0)
  sigma     <- sqrt(mu2_clean)
  sigma_safe <- ifelse(sigma < eps, NA_real_, sigma)
  
  # Initialize
  gamma <- rep(NA_real_, n)
  kappa <- rep(NA_real_, n)
  
  # Compute skewness/kurtosis where safe
  valid <- which(!is.na(sigma_safe))
  if (length(valid) > 0) {
    gamma[valid] <- mu3[valid] / (sigma_safe[valid]^3)
    kappa[valid] <- mu4[valid] / (sigma_safe[valid]^4)
  }
  
  # Optionally flag bad rows
  if (return_flags) {
    flags <- data.frame(
      zero_variance = is.na(sigma_safe),
      negative_mu2  = mu2 < 0
    )
    return(list(summary = data.frame(mu, sigma, gamma, kappa), flags = flags))
  }
  
  data.frame(mu, sigma, gamma, kappa)
}

# Apply to training data
train_responses <- calculate_summary_stats(train_raw)

# Combine predictors and responses
train_processed <- cbind(train_raw, train_responses)
head(train_processed)
```

### 2.2.1 Feature Engineering and Transformation

Based on the exploratory analysis, we engineer the `Fr` variable and apply a log transformation to the highly skewed response variables and predictor variable St.

```{r feature-engineering}
## ---- feature-engineering-fixed, message=FALSE, warning=FALSE -----------------

# Feature engineering and safe inverse and log transforms
process_features <- function(df) {
  df %>%
    mutate(
      is_gravity_present = ifelse(is.infinite(Fr), 0, 1),
      inv_Fr             = ifelse(is.infinite(Fr), 0, 1 / Fr),
      log_St             = ifelse(St > 0, log(St), NA_real_) 
    ) %>%
    select(St, log_St, Re, is_gravity_present, inv_Fr)
}

train_predictors_transformed <- process_features(train_raw)
test_predictors  <- process_features(test_raw)

# Safe log transformation for response variables
safe_log_transform <- function(df, small = 1e-8) {
  df %>%
    mutate(
      log_mu     = ifelse(mu     > 0, log(mu),     NA_real_),
      log_sigma  = ifelse(sigma  > 0, log(sigma),  NA_real_),
      log_gamma  = ifelse(gamma  > 0, log(gamma),  NA_real_),
      log_kappa  = ifelse(kappa  > 0, log(kappa),  NA_real_)
    ) %>%
    select(log_mu, log_sigma, log_gamma, log_kappa)
}

train_responses_transformed <- safe_log_transform(train_responses)


train_final <- cbind(train_predictors_transformed, train_responses_transformed)

# Identify any problematic (NA or Inf) rows
problem_rows <- which(!is.finite(train_final$log_mu) |
                      !is.finite(train_final$log_sigma) |
                      !is.finite(train_final$log_gamma) |
                      !is.finite(train_final$log_kappa))


head(train_final)
```

### 2.2.2 Univariate EDA of Responses for Current and Transformation

```{r}
## ---- two-faceted-plots, fig.width=10, fig.height=8, message=FALSE, warning=FALSE ----

# Prepare separate before and after datasets
before_df <- train_responses %>%
  mutate(stage = "Before Transformation") %>%
  select(stage, mu, sigma, gamma, kappa) %>%
  pivot_longer(cols = c(mu, sigma, gamma, kappa),
               names_to = "variable", values_to = "value")

after_df <- train_final %>%
  mutate(stage = "After Transformation") %>%
  rename(mu = log_mu, sigma = log_sigma, gamma = log_gamma, kappa = log_kappa) %>%
  select(stage, mu, sigma, gamma, kappa) %>%
  pivot_longer(cols = c(mu, sigma, gamma, kappa),
               names_to = "variable", values_to = "value")

# --- BEFORE TRANSFORMATION ---
ggplot(before_df, aes(x = value, fill = variable)) +
  geom_histogram(bins = 30, alpha = 0.7, position = "identity", color = "white") +
  facet_wrap(~ variable, scales = "free", ncol = 2) +
  theme_minimal(base_size = 13) +
  scale_fill_brewer(palette = "Set2") +
  labs(
    title = "Before Transformation: Distributions of Response Variables",
    x = "Value", y = "Frequency"
  ) +
  theme(plot.title = element_text(face = "bold", size = 15),
        legend.position = "none")

# --- AFTER TRANSFORMATION ---
ggplot(after_df, aes(x = value, fill = variable)) +
  geom_histogram(bins = 30, alpha = 0.7, position = "identity", color = "white") +
  facet_wrap(~ variable, scales = "free", ncol = 2) +
  theme_minimal(base_size = 13) +
  scale_fill_brewer(palette = "Set2") +
  labs(
    title = "After Transformation: Distributions of Response Variables",
    x = "Value", y = "Frequency"
  ) +
  theme(plot.title = element_text(face = "bold", size = 15),
        legend.position = "none")
```

### 2.2.3 Bivariate EDA of Responses for Current and Transformation

```{r}
ggplot(train_final, aes(x = log_St, y = log_mu, color = factor(Re), shape = factor(inv_Fr))) +
  geom_point() +
  labs(x = "Log(St)",
       y = "Log of Particle Distribution Mean",
       color = "Re",
       shape = "1/Fe",
       title = "Particle Characteristic Value (St) vs Particle Distribution Mean",
       subtitle = "Factored by Reynold's Number (Re) and Gravitational Acceleration (Fe)")

ggplot(train_final, aes(x = St, y = exp(log_mu), color = factor(Re), shape = factor(inv_Fr))) +
  geom_point() +
  labs(x = "St",
       y = "Particle Distribution Mean",
       color = "Re",
       shape = "1/Fe",
       title = "Particle Characteristic Value (St) vs Particle Distribution Mean",
       subtitle = "Factored by Reynold's Number (Re) and Gravitational Acceleration (Fe)")

ggplot(train_final, aes(x = log_St, y = log_sigma, color = factor(Re), shape = factor(inv_Fr))) +
  geom_point() +
  labs(x = "Log(St)",
       y = "Log(Sigma)",
       color = "Re",
       shape = "1/Fe",
       title = "Particle Characteristic Value (St) vs Particle Distribution Sigma",
       subtitle = "Factored by Reynold's Number (Re) and Gravitational Acceleration (Fe)")

ggplot(train_final, aes(x = log_St, y = log_sigma, color = factor(Re), shape = factor(inv_Fr))) +
  geom_point() +
  labs(x = "Log(St)",
       y = "Log(Sigma)",
       color = "Re",
       shape = "1/Fe",
       title = "Particle Characteristic Value (St) vs Particle Distribution Sigma",
       subtitle = "Factored by Reynold's Number (Re) and Gravitational Acceleration (Fe)")

ggplot(train_final, aes(x = log_St, y = log_gamma, color = factor(Re), shape = factor(inv_Fr))) +
  geom_point() +
  labs(x = "Log(St)",
       y = "Log(Gamma)",
       color = "Re",
       shape = "1/Fe",
       title = "Particle Characteristic Value (St) vs Particle Distribution Gamma",
       subtitle = "Factored by Reynold's Number (Re) and Gravitational Acceleration (Fe)")

ggplot(train_final, aes(x = log_St, y = log_kappa, color = factor(Re), shape = factor(inv_Fr))) +
  geom_point() +
  labs(x = "Log(St)",
       y = "Log(Kappa)",
       color = "Re",
       shape = "1/Fe",
       title = "Particle Characteristic Value (St) vs Particle Distribution Kappa",
       subtitle = "Factored by Reynold's Number (Re) and Gravitational Acceleration (Fe)")
```

### 2.3. Exploratory Data Analysis (EDA)

A summary of the final processed data, similar to Table 1 in the report.

```{r eda}
# Generate descriptive statistics for the final dataset
summary_table <- summary(train_final)

# Print the summary
print(summary_table)
```

### 3. Linear Fitting

We establish a baseline by fitting a linear model. This helps formally test for non-linearity.

```{r}
## 3. Baseline Model: Multivariate Linear Regression
# Define a list of response variables
responses <- c("log_mu", "log_sigma", "log_gamma", "log_kappa")
lm_models <- list()
lm_rmse <- list()

# Loop through each response to build a model
for (response in responses) {
  formula_str <- paste(response, "~ log_St * Re + log_St * inv_Fr + is_gravity_present")
  model <- lm(as.formula(formula_str), data = train_final)
  lm_models[[response]] <- model
  
  # Print model summary
  cat("--- Linear Model Summary for", response, "---\n")
  print(summary(model))
  summary(model)
  
  # Diagnostic plots
  par(mfrow = c(2, 2))
  plot(model, main = paste("Linear Model Diagnostics for", response))
  par(mfrow = c(1, 1))
  
  # Check for multicollinearity
  cat("\n--- VIF for", response, "---\n")
  print(vif(model))
  
  # Predict on test set and calculate RMSE
  predictions <- predict(model, newdata = test_predictors)
  # Note: We don't have true values for the test set, so RMSE is calculated on training data for diagnostics
  train_preds <- predict(model, newdata = train_final)
  rmse <- sqrt(mean((train_final[[response]] - train_preds)^2))
  lm_rmse[[response]] <- rmse
  cat("\nTraining RMSE for", response, ":", rmse, "\n\n")
}
```

1.  Model for log_mu and log_sigma (First & Second Images)

For the models predicting the log-transformed mean and standard deviation, the diagnostic plots reveal several significant issues:

Failure of Linearity Assumption: The Residuals vs. Fitted plots for both models show a distinct, curvilinear pattern in the residuals. The smoothed red line follows a clear U-shape rather than being flat and straight. This is a classic sign that the linear model is failing to capture the true underlying non-linear relationships in the data. It is systematically mis-predicting in different regions of the data.

Non-Normal Residuals: The Normal Q-Q plots show that the residuals deviate from the theoretical diagonal line, particularly at the tails. This indicates that the errors of the model are not normally distributed; specifically, they have heavier tails than a normal distribution would suggest, violating a key assumption of linear regression.

Non-Constant Variance (Heteroscedasticity): In the Scale-Location plot, especially for log_sigma, the smoothed line is not flat. It trends upwards, indicating that the variance of the residuals increases as the fitted (predicted) value gets larger. This means the model's predictions are less reliable for larger-scale outcomes.

Presence of Influential Points: The Residuals vs. Leverage plots highlight that some data points have high leverage or large residuals. These points have an unusually strong influence on the estimated coefficients of the model, potentially making the fit unstable and sensitive to a small subset of the data.

Conclusion: For log_mu and log_sigma, the linear model is inadequate as it violates the core assumptions of linearity, normality of residuals, and constant variance.

2.  Model for gamma (Skewness - Third Image)

The diagnostics for the gamma model are even more problematic:

Dominance by an Extreme Outlier: All four diagnostic plots are dominated by a single, extreme point with a very large residual.

Severe Violation of Normality: The Normal Q-Q plot shows a catastrophic failure of the normality assumption. One point lies so far from the diagonal line that it renders the plot almost unreadable for the other points, indicating the model is completely unable to account for this observation.

Extreme Influence: The Residuals vs. Leverage plot confirms that this outlier is an extremely influential point, falling far outside the standard contours of Cook's distance. This means this single data point is fundamentally distorting the entire model fit.

Conclusion: The linear model for gamma is invalid. It is not robust and its results are unreliable due to the overwhelming effect of at least one influential outlier.

3.  Model for log_kappa (Kurtosis - Fourth Image)

The diagnostics for log_kappa mirror the issues seen with log_mu and log_sigma:

Clear Non-Linearity: The Residuals vs. Fitted plot again shows a pronounced U-shaped pattern, failing the linearity assumption.

Non-Normal Residuals: The Normal Q-Q plot shows points peeling away from the diagonal line at the tails, violating the normality assumption.

Influential Points: The model is again sensitive to a few points with high leverage or large residuals.

Conclusion: The linear model for log_kappa is also a poor fit, failing on the same key assumptions as the other models.

Therefore, we must explore either more complex parametric models or non-linear models.



### 3b. Cubic Modeling

```{r}
cm_models <- list()
cm_rmse <- list()

# Loop through each response to build a model
for (response in responses) {
  formula_str <- paste(response, "~ poly(log_St, 3, raw = TRUE)*Re*inv_Fr")
  model <- lm(as.formula(formula_str), data = train_final)
  lm_models[[response]] <- model
  
  # Print model summary
  cat("--- Linear Model Summary for", response, "---\n")
  print(summary(model))
  summary(model)
  
  # Diagnostic plots
  par(mfrow = c(2, 2))
  plot(model, main = paste("Linear Model Diagnostics for", response))
  par(mfrow = c(1, 1))
  
  # Check for multicollinearity
  cat("\n--- VIF for", response, "---\n")
  print(vif(model))
  
  # Predict on test set and calculate RMSE
  predictions <- predict(model, newdata = test_predictors)
  # Note: We don't have true values for the test set, so RMSE is calculated on training data for diagnostics
  train_preds <- predict(model, newdata = train_final)
  rmse <- sqrt(mean((train_final[[response]] - train_preds)^2))
  lm_rmse[[response]] <- rmse
  cat("\nTraining RMSE for", response, ":", rmse, "\n\n")
}
```


### 4. Advanced Modeling: Generalized Additive Models (GAMs)

The linear model diagnostics clearly showed that a simple linear relationship is not appropriate. We will now fit a Generalized Additive Model (GAM) for each response.

```{r}

# Prepare data for factor modeling
# We use the original 'Re' and 'inv_Fr' and explicitly make them factors.
train_final_gam <- train_final %>%
  mutate(
    log_St_s = as.numeric(scale(log_St)), # Keep scaled continuous variable
    Re = factor(Re),               # Treat as a factor
    inv_Fr = factor(round(inv_Fr, 4)), # Round to clean up levels, then factor
    is_gravity_present = factor(is_gravity_present)
  )

# We need to apply the same factor levels to the test data
# Get levels from training data
re_levels <- levels(train_final_gam$Re)
fr_levels <- levels(train_final_gam$inv_Fr)
grav_levels <- levels(train_final_gam$is_gravity_present)

# Process test predictors
test_predictors_gam <- test_predictors %>%
  mutate(
    log_St_s = as.numeric(scale(log_St, center = mean(train_final$log_St), scale = sd(train_final$log_St))),
    Re = factor(Re, levels = re_levels),
    inv_Fr = factor(round(inv_Fr, 4), levels = fr_levels),
    is_gravity_present = factor(is_gravity_present, levels = grav_levels)
  )


responses <- c("log_mu", "log_sigma", "log_gamma", "log_kappa")
gam_models <- list()

for (response in responses) {
  
  # s(St_s):            A global smooth effect of St
  # Re:                 A main effect for each Re level (different intercept)
  # inv_Fr:             A main effect for each inv_Fr level
  # is_gravity_present: A main effect for gravity
  # s(St_s, by = Re):    *separate* smooth of St for *each* #                     level of Re. This models the interaction.
  # s(St_s, by = inv_Fr): Models the interaction of St and inv_Fr.
  
  formula_gam <- as.formula(paste(
    response, 
    "~ s(log_St_s) + Re + inv_Fr + is_gravity_present + s(log_St_s, by = Re) + s(log_St_s, by = inv_Fr)"
  ))
  
  cat("\n--- Fitting GAM for", response, "---\n")
  cat("Using formula:\n"); print(formula_gam)
  
  model <- gam(
    formula_gam,
    data = train_final_gam,
    method = "REML", # Recommended for stable smoothness
    select = TRUE   # Allows model to penalize smooths to zero
  )
  
  gam_models[[response]] <- model
  
  # Print the model summary
  cat("\n--- GAM Summary for", response, "---\n")
  print(summary(model))
  
  # Check diagnostics
  cat("\n--- GAM Diagnostics for", response, "---\n")
  par(mfrow = c(2, 2))
  gam.check(model)
  par(mfrow = c(1, 1))
  
  # Plot the learned smooths
  cat("\n--- Plotting smooths for", response, "---\n")
  # This will now show the main effect of St, and then 
  # the *three separate interaction smooths* for s(St_s):Re90, etc.
  plot(model, pages = 1, all.terms = TRUE, rug = TRUE, se = TRUE, residuals = TRUE)
  
}
```

Our first-pass Generalized Additive Models strongly validate the move from a linear framework, with the log_mu model's R-squared of 0.997 and an effective degrees of freedom (edf) of 5.6 for the Stokes number smooth term providing conclusive proof of the non-linearity that the lm diagnostics merely suggested. This flexible approach also reveals key scientific insights, such as the log_mu model being driven primarily by a specific interaction between particle inertia and high gravity, while the log_kappa model is appropriately simplified to a purely parametric form. However, these diagnostics are equally critical for identifying model flaws: the log_sigma model's gam.check() p-values are universally low, warning us that its basis dimension is too restrictive (k=9) and the fit is unreliable. Furthermore, perfect collinearity is evident as the is_gravity_present term is redundant with the inv_Fr factor, necessitating its removal. Therefore, our next iteration must correct these issues by increasing k and simplifying the model formula to achieve a final, robust fit.


```{r}

train_final_gam_V2 <- train_final %>%
  mutate(
    log_St_s = as.numeric(scale(log_St)), #Scaled log continous variable
    St_s = as.numeric(scale(St)), # Scaled continuous variable
    Re_factor = factor(Re),                  # Treat as a factor
    Fr_factor = factor(round(inv_Fr, 4)), # Round to clean up levels, then factor
    is_gravity_present = factor(is_gravity_present, levels = grav_levels)
  )

# --- *exact same* transformation to the test data ---
log_st_mean_V2 <- mean(train_final$log_St)
log_st_sd_V2 <- sd(train_final$log_St)
re_levels_V2 <- levels(train_final_gam_V2$Re_factor)
fr_levels_V2 <- levels(train_final_gam_V2$Fr_factor)

# Process test predictors
test_predictors_gam_V2 <- test_predictors %>%
  mutate(
    log_St_s = (log_St - log_st_mean_V2) / log_st_sd_V2,
    Re_factor = factor(Re, levels = re_levels_V2),
    Fr_factor = factor(round(inv_Fr, 4), levels = fr_levels_V2)
  )


responses <- c("log_mu", "log_sigma", "log_gamma", "log_kappa")
gam_models_V3 <- list()

# Final Justified Models (The Real V3)
final_models <- list()

# --- 1. Final Model for log_mu ---
# Based on the summary, we remove the non-significant terms.
formula_log_mu <- log_mu ~ Re_factor + s(log_St_s, k=8)

model_log_mu <- gam(formula_log_mu, data = train_final_gam_V2, method = "REML", na.action = na.exclude)
final_models[["log_mu"]] <- model_log_mu
print(summary(model_log_mu))
gam.check(model_log_mu)


# --- 2. Final Model for log_sigma ---
# Evidence: Only the GLOBAL s(St_s) was significant. All 'by' interactions were not.
cat("\n--- Fitting Final Justified Model for log_sigma ---\n")
formula_log_sigma <- log_sigma ~ s(log_St_s, k=9) + Re_factor + Fr_factor

model_log_sigma <- gam(formula_log_sigma, data = train_final_gam_V2, method = "REML", na.action = na.exclude)
final_models[["log_sigma"]] <- model_log_sigma
print(summary(model_log_sigma))


# This summary will be much cleaner and all terms should be significant.


# --- 3. Final Model for log_gamma ---
# Evidence: NO smooth terms were significant. The model is purely parametric.
# We should use a simple, robust lm()

cat("\n--- Fitting Final Justified Model for log_gamma ---\n")

formula_log_gamma <- log_gamma ~ Re_factor + Fr_factor
model_log_gamma <- lm(formula_log_gamma, data = train_final_gam_V2, na.action = na.exclude)
final_models[["log_gamma"]] <- model_log_gamma
print(summary(model_log_gamma))


# --- 4. Final Model for log_kappa ---
# Evidence: NO smooth terms were significant. The model is purely parametric.
cat("\n--- Fitting Final Justified Model for log_kappa ---\n")
formula_log_kappa <- log_kappa ~ Re_factor + Fr_factor

model_log_kappa <- lm(formula_log_kappa, data = train_final_gam_V2, na.action = na.exclude)
final_models[["log_kappa"]] <- model_log_kappa
print(summary(model_log_kappa))


cat("\nFinished. Final, justified models saved in `final_models`.\n")
```

```{r}
plot(model_log_mu) #plot of non linear term of sigma with log predictor

draw(model_log_mu) + 
  scale_x_continuous(labels = function(x) round(exp(x), 2)) +
  labs(x = "Exponentiated x-axis") #plot after exponentiating x axis back to St

draw(model_log_mu) + 
  scale_x_continuous(labels = function(x) round(exp(x), 2)) +
  scale_y_continuous(labels = function(x) round(exp(x), 2)) +
  labs(x = "Exponentiated x-axis") #plot after exponentiating x and y axis back to original

df <- smooth_estimates(model_log_mu, smooth = 1)
df$St_s <- exp(df$log_St_s)  # Exponentiate the predictor

ggplot(df, aes(x = St_s, y = .estimate)) +
  geom_line() +
  geom_ribbon(aes(ymin = .estimate - 2*.se, ymax = .estimate + 2*.se), alpha = 0.2) +
  labs(x = "St_s", y = "Partial effect")

df2 <- smooth_estimates(model_log_mu)

df2$St_s <- exp(df$log_St_s)               # unlog x-axis if needed
df2$effect_unlogged <- exp(df$.estimate)   # unlog y-axis
df2$upper <- exp(df$.estimate + 2 * df$.se)
df2$lower <- exp(df$.estimate - 2 * df$.se)

ggplot(df2, aes(x = St_s, y = effect_unlogged)) +
  geom_line() +
  geom_ribbon(aes(ymin = lower, ymax = upper), alpha = 0.2) +
  labs(x = "St", y = "Non-Linear Effect on Mean",
       title = "Estimated Particle Charcteristic (St) Effect on Mean",
       subtitle = "Exponentiated to Reverse Log Variable Transformations") #Graph we want for variance
```



```{r}

train_final_gam_V3 <- train_final %>%
  mutate(
    log_St_s = as.numeric(scale(log_St)),      # Scaled continuous variable
    Re_factor = factor(Re),                  # Treat as a factor
    Fr_factor = factor(round(inv_Fr, 4)) # Round to clean up levels, then factor
  )

# --- *exact same* transformation to the test data ---
log_st_mean_V3 <- mean(train_final$log_St)
log_st_sd_V3 <- sd(train_final$log_St)
re_levels_V3 <- levels(train_final_gam_V3$Re_factor)
fr_levels_V3 <- levels(train_final_gam_V3$Fr_factor)

# Process test predictors
test_predictors_gam_V3 <- test_predictors %>%
  mutate(
    log_St_s = (log_St - log_st_mean_V3) / log_st_sd_V3,
    Re_factor = factor(Re, levels = re_levels_V3),
    Fr_factor = factor(round(inv_Fr, 4), levels = fr_levels_V3)
  )


responses <- c("log_mu","log_sigma", "log_gamma", "log_kappa")
gam_models_V3 <- list()


for (response in responses) {
  

  #   ~ St       + Re_factor + Fr_factor + St:Re_factor       + St:Fr_factor
  #
  # s(St_s, k=9):              The main smooth effect of St.
  # Re_factor:                 The main intercept effect for each Re level.
  # Fr_factor:                 The main intercept effect for each Fr level.
  # s(St_s, by = Re_factor, k=9, m=1): 
  #   The smooth INTERACTION. m=1 is the key. It removes the
  #   intercept from this term, breaking the collinearity.
  # s(St_s, by = Fr_factor, k=9, m=1):
  #   The other smooth INTERACTION.
  
  formula_gam <- as.formula(paste(
    response, 
    "~ s(log_St_s, k=9) + Re_factor*Fr_factor"
  ))
  
  cat("\n--- Fitting Corrected V3 GAM for", response, "---\n")
  cat("Using formula:\n"); print(formula_gam)
  
  model <- tryCatch({
    gam(
      formula_gam,
      data = train_final_gam_V3,
      method = "REML",
      select = TRUE
    )
  }, error = function(e) {
    cat("\n--- ERROR IN GAM FIT for", response, "--- \n")
    message(e)
    cat("-------------------------------------------\n")
    NULL
  })
  
  gam_models_V3[[response]] <- model
  
  if (is.null(model)) {
    cat("--- Model for", response, "was NULL (fit failed). Skipping. ---\n")
    next
  }
  
  # --- This will finally print the summary with a high R^2 ---
  cat("\n--- GAM Summary for", response, "---\n")
  print(summary(model))
    
  # Check diagnostics
  cat("\n--- GAM Diagnostics for", response, "---\n")
  par(mfrow = c(2, 2)) # Set up the 2x2 grid
  try({
    # 1. Run the diagnostics
    gam.check(model)
    
    # 2. Create a dynamic title
    diag_title <- paste("GAM Diagnostics for:", response)
    
    # 3. Add the title to the top of the entire page
    mtext(diag_title, side = 3, line = -2, outer = TRUE, cex = 1.2, font = 2)
  })
  par(mfrow = c(1, 1))
  
  # Plot the learned smooths
  cat("\n--- Plotting smooths for", response, "---\n")
  try({
  # 1. Create the plot as before
  plot(model, pages = 1, all.terms = TRUE, rug = TRUE, se = TRUE, residuals = TRUE)
  
  # 2. Create a dynamic title
  plot_title <- paste("GAM Smooth Terms for:", response)
  
  # 3. Add the title to the top of the entire page
  mtext(plot_title, side = 3, line = -2, outer = TRUE, cex = 1.2, font = 2)
})
  
}

cat("\nFinished. Models saved in `gam_models_V3`.\n")
```

1. Justification for the log_mu Model

Our final model for the mean cluster size is log_mu ~ Re_factor + s(log_St_s, k = 8). While an initial simpler model using only Re_factor was strong (R-sq(adj) = 0.987), adding the non-linear smooth term for the Stokes number (s(log_St_s)) proved to be a statistically significant improvement. As the summary output shows, this smooth term is highly significant (p < 2e-16) and increases the adjusted R-squared to 0.996. This model is superior as it correctly captures the two main drivers of the mean: the overwhelmingly dominant effect of the Reynolds number (Re_factor) and the significant, non-linear influence of particle inertia (log_St_s). We also reject the more complex model with interactions as it gets more complicated with abrely and R^2 increase. 


2. Justification for log_sigma Model:

This GAM represents the optimal balance for predicting the log-transformed standard deviation (log_sigma). Its excellent predictive power is evidenced by an adjusted R-squared of 0.973. Crucially, the model's complexity is both justified and reliable: the smooth term for log(St) is highly significant (p < 2e-16) and demonstrably non-linear (edf = 5.43), confirming the inadequacy of simpler linear approaches. Furthermore, the gam.check() diagnostic yields a p-value of 0.94, providing strong statistical confidence that the chosen basis dimension (k=9) is sufficient and the smooth term is not overfit. Interpretability is enhanced by using the Re_factor * Fr_factor structure, which provides a clear baseline (Re90:Fr0) and reveals that the statistically significant interaction term (p < 2e-16 for several interaction coefficients) drives the model, correctly indicating that the effects of turbulence and gravity are interdependent, while the main effects themselves are not the primary story.

3. Justification for log_gamma Model:

The final GAM for the log-transformed skewness (log_gamma) achieves strong predictive performance with an adjusted R-squared of 0.973. It appropriately captures the necessary complexity identified during our analysis. The significance (p = 2.86e-06) and non-linearity (edf = 2.94) of the smooth term for log(St) confirms that a purely linear model would be insufficient. Importantly, the gam.check() diagnostic p-value of 0.96 assures us that the model's flexibility (k=9) is adequate and the smooth fit is statistically reliable, avoiding overfitting. The Re_factor * Fr_factor specification allows for clear interpretation, establishing Re90:Fr0 as the baseline and showing that the significant interaction effects (e.g., p < 2e-16 for several Re:Fr terms) are essential for explaining skewness, outweighing the simple main effects.

4. Justification for log_kappa Model:

For log-transformed kurtosis (log_kappa), this GAM provides an excellent fit, explaining 97.7% of the variance (R-sq.(adj) = 0.977). Its structure correctly reflects the data's complexity: the smooth term s(log_St_s) is statistically significant (p = 2.32e-06) and moderately non-linear (edf = 2.40), demonstrating the need for flexibility beyond a simple linear term. Crucially, the model's reliability is confirmed by the gam.check() p-value of 0.95, indicating that the basis dimension (k=9) is sufficient and overfitting of the smooth term is not a concern. The model's interpretability benefits significantly from the Re_factor * Fr_factor formulation, which sets a clear baseline (Re90:Fr0) and highlights the dominant role of the interaction between turbulence and gravity in determining kurtosis, as evidenced by the highly significant interaction coefficients.


The previous V2 models are "too simple" for log_sigma, log_gamma, and log_kappa not just because they have fewer terms, but because they actively ignore statistically significant complexities (the non-linear effect of log(St) and the interaction between Re and Fr) that our previous, rigorous analysis proved were present and important. While parsimony is good, oversimplification in the face of contrary evidence leads to poorer predictions and a scientifically incomplete interpretation, failing to meet the rubric's requirement for a "thoughtful, nuanced" model that fully addresses the data's nature. Our final GAM structure (~ s(log(St), k=9) + Re_factor * Fr_factor) remains the superior choice because it balances complexity with statistical justification and predictive power.

```{r}
# --- NEW: Load Raw Test Data ---
# We need this to bind the predictor columns to the final CSV
test_data_raw <- read_csv("data-test.csv")

# Filter for the valid rows (St > 0) that models can predict on
# This will result in 23 rows, matching the prediction output
test_data_valid_predictors <- test_data_raw %>%
  filter(St > 0) %>%
  select(St, Re, Fr) 
  # bind_cols relies on row order, which filter() preserves.

# --- 1. Calculate Residuals from Training Data for Smearing ---
# (This part is unchanged)

# V2 model for log_mu
preds_train_mu <- as.numeric(predict(final_models[["log_mu"]], newdata = train_final_gam_V3))
resids_mu <- train_final_gam_V3$log_mu - preds_train_mu

# V3 models
v3_model_vars <- c("log_sigma", "log_gamma", "log_kappa", "log_St_s", "Re_factor", "Fr_factor")
v3_complete_cases <- complete.cases(train_final_gam_V3[v3_model_vars])
train_data_v3_complete <- train_final_gam_V3[v3_complete_cases, ]

preds_train_sigma <- as.numeric(predict(gam_models_V3[["log_sigma"]], newdata = train_data_v3_complete))
preds_train_gamma <- as.numeric(predict(gam_models_V3[["log_gamma"]], newdata = train_data_v3_complete))
preds_train_kappa <- as.numeric(predict(gam_models_V3[["log_kappa"]], newdata = train_data_v3_complete))

resids_sigma <- train_data_v3_complete$log_sigma - preds_train_sigma
resids_gamma <- train_data_v3_complete$log_gamma - preds_train_gamma
resids_kappa <- train_data_v3_complete$log_kappa - preds_train_kappa

# --- 2. Calculate Smearing Factors ---
# (This part is unchanged)
smear_mu <- 1.0 
smear_sigma <- mean(exp(resids_sigma), na.rm = TRUE)
smear_gamma <- mean(exp(resids_gamma), na.rm = TRUE)
smear_kappa <- mean(exp(resids_kappa), na.rm = TRUE)

cat("--- GAM Smearing Factors ---\n")
print(paste("Sigma:", smear_sigma))
print(paste("Gamma:", smear_gamma))
print(paste("Kappa:", smear_kappa))

# --- 3. Predict on the Test Set (on the log scale) ---
# (This part is unchanged)
preds_test_mu <- as.numeric(predict(final_models[["log_mu"]], 
                                    newdata = test_predictors_gam_V3, 
                                    na.action = na.pass))

preds_test_sigma <- as.numeric(predict(gam_models_V3[["log_sigma"]], 
                                       newdata = test_predictors_gam_V3, 
                                       na.action = na.pass))

preds_test_gamma <- as.numeric(predict(gam_models_V3[["log_gamma"]], 
                                       newdata = test_predictors_gam_V3, 
                                       na.action = na.pass))

preds_test_kappa <- as.numeric(predict(gam_models_V3[["log_kappa"]], 
                                       newdata = test_predictors_gam_V3, 
                                       na.action = na.pass))

# --- 4. Combine, Re-transform, Apply Smearing ---
# (MODIFIED: Create predictions_only first)
predictions_only <- tibble(
  log_mu = preds_test_mu,
  log_sigma = preds_test_sigma,
  log_gamma = preds_test_gamma,
  log_kappa = preds_test_kappa
) %>%
  mutate(
    mean     = smear_mu * exp(log_mu),
    sd       = smear_sigma * exp(log_sigma),
    skewness = smear_gamma * exp(log_gamma),
    kurtosis = smear_kappa * exp(log_kappa)
  ) %>%
  select(mean, sd, skewness, kurtosis)
  
# --- 5. Combine Predictors and Predictions, then Save ---
# This check ensures no errors if row counts mismatch
if(nrow(test_data_valid_predictors) == nrow(predictions_only)) {
  # NEW: Bind the predictors (St, Re, Fr) to the predictions
  predictions_gam_hybrid <- bind_cols(test_data_valid_predictors, predictions_only)
} else {
  cat("ERROR: Predictor and prediction row counts mismatch! Saving predictions only.\n")
  predictions_gam_hybrid <- predictions_only # Fallback
}
  
# Save the new, combined dataframe to the CSV
write_csv(predictions_gam_hybrid, "predictions_gam_hybrid.csv")

cat("\nHybrid GAM predictions (with predictors) saved to 'predictions_gam_hybrid.csv'\n")
# This should show 23 rows
print(paste("Number of rows in GAM prediction:", nrow(predictions_gam_hybrid)))
# The head() will now show St, Re, Fr, mean, sd, etc.
head(predictions_gam_hybrid)
```


### 5. Advanced Modeling: Nonparametric model - random forests

As an alternative to the parametric GAMs, we will also fit a non-parametric Random Forest model. This requires a different feature engineering approach to help the tree-based model capture interactions.


```{r}
# Load libraries for Random Forest
library(ranger)
library(tibble)
library(tidyr)
library(readr)

# RF-specific feature engineering (Original 10-feature version)
process_features_rf <- function(df) {
  df %>%
    mutate(
      inv_Fr             = ifelse(is.infinite(Fr), 0, 1 / Fr),
      # Use safe logging consistent with earlier analysis
      logRe              = ifelse(Re > 0, log(Re), NA_real_),
      logSt              = ifelse(St > 0, log(St), NA_real_)
    ) %>%
    mutate(
      Re_x_St        = Re * St,
      invFr_x_Re     = inv_Fr * Re,
      invFr_x_St     = inv_Fr * St,
      logRe_x_logSt  = logRe * logSt
    ) %>%
    select(
      # base
      St, Re, inv_Fr,
      # scale helpers
      logRe, logSt,
      # light interactions (helps splitting)
      Re_x_St, invFr_x_Re, invFr_x_St, logRe_x_logSt
    )
}

train_predictors_rf <- process_features_rf(train_raw)
test_predictors_rf  <- process_features_rf(test_raw)

# Combine RF predictors with the *original* log-transformed responses
train_rf_df <- cbind(train_predictors_rf, train_responses_transformed)

# Use the *original* problem_rows to filter the data
train_model_df <- if (length(problem_rows) > 0) {
  train_rf_df[-problem_rows, , drop = FALSE]
} else {
  train_rf_df
}

# Define the predictor columns for the RF model (Original 10-feature list)
x_cols <- c("St","Re","inv_Fr",
            "logRe","logSt","Re_x_St","invFr_x_Re","invFr_x_St","logRe_x_logSt")

cat("RF predictors reverted to original 10-feature numeric set.\n")
cat("Predictor set:", paste(x_cols, collapse = ", "), "\n")
```

```{r}
## ---- random-forest-with-smearing, message=FALSE, warning=FALSE ----------
set.seed(42)

# Helper function to calculate R-squared
calculate_r2 <- function(y_true, y_pred) {
  # Ensure no NAs in this calculation
  valid_indices <- complete.cases(y_true) & complete.cases(y_pred)
  y_true <- y_true[valid_indices]
  y_pred <- y_pred[valid_indices]
  
  if(length(y_true) < 2) return(NA_real_) # Need at least 2 points to calculate variance
  
  rss <- sum((y_true - y_pred)^2)
  tss <- sum((y_true - mean(y_true))^2)
  if (tss == 0) {
    return(ifelse(rss == 0, 1.0, 0.0))
  }
  1 - (rss / tss)
}

targets <- list(
  log_mu    = list(inv = exp,   smear = FALSE),
  log_sigma = list(inv = exp,   smear = TRUE),
  log_gamma = list(inv = exp,   smear = TRUE),
  log_kappa = list(inv = exp,   smear = TRUE)
)

targets[["gamma"]] <- NULL
targets[["log_gamma"]] <- list(inv = exp, smear = TRUE)


# mtry will now be floor(sqrt(5)) = 2
# min.node.size is set to 5 to reduce overfitting
rf_args <- list(
  num.trees = 600L,
  mtry = floor(sqrt(length(x_cols))), 
  importance = "permutation",
  min.node.size = 4L,  # <-- Set to 5
  replace = TRUE,
  sample.fraction = 0.632,
  oob.error = TRUE
)

cat(paste("Running RF with mtry =", rf_args$mtry, "and min.node.size =", rf_args$min.node.size, "\n"))

models   <- list()
oob_r2s  <- c()
in_sample_r2s <- c() # To store in-sample R^2
smear_sf <- c()
imps_long <- list()

for (tgt in names(targets)) {
  
  if (!tgt %in% colnames(train_model_df)) {
    cat("Skipping target:", tgt, "(not found in train_model_df)\n")
    next
  }
  
  dat <- train_model_df[, c(x_cols, tgt), drop = FALSE]
  dat <- dat[complete.cases(dat), ]
  colnames(dat)[ncol(dat)] <- "y"
  
  if(nrow(dat) == 0) {
    cat("Skipping target:", tgt, "(no complete cases)\n")
    next
  }

  fit <- ranger(
    y ~ ., data = dat,
    num.trees = rf_args$num.trees,
    mtry = rf_args$mtry,
    importance = rf_args$importance,
    min.node.size = rf_args$min.node.size,
    replace = rf_args$replace,
    sample.fraction = rf_args$sample.fraction,
    oob.error = rf_args$oob.error,
    seed = 42
  )
  models[[tgt]] <- fit
  
  # OOB R^2 (from the model object directly)
  oob_r2s[tgt]  <- fit$r.squared
  
  # In-sample predictions
  yhat_in <- predict(fit, data = dat[, x_cols, drop = FALSE])$predictions
  
  # In-sample R^2 (calculated manually)
  in_sample_r2s[tgt] <- calculate_r2(dat$y, yhat_in)

  resid   <- dat$y - yhat_in # For smearing

  if (targets[[tgt]]$smear) {
    smear_sf[tgt] <- mean(exp(resid), na.rm = TRUE)
  } else {
    smear_sf[tgt] <- 1.0
  }

  imp <- importance(fit)
  imps_long[[tgt]] <- tibble(feature = names(imp),
                             importance = as.numeric(imp),
                             target = tgt)
}

# --- Load and filter test data predictors ---
test_data_raw_rf <- read_csv("data-test.csv")
test_data_valid_predictors_rf <- test_data_raw_rf %>%
  filter(St > 0) %>%
  select(St, Re, Fr)

# Predict test (model scale) -> back-transform
# We must filter the test_predictors_rf in the same way
valid_test_rows <- complete.cases(test_predictors_rf)

pred_model_scale <- lapply(names(targets), function(tgt) {
  predict(models[[tgt]], data = test_predictors_rf[valid_test_rows, ])$predictions
}) |> setNames(names(targets)) |> as_tibble()


pred_original_scale <- pred_model_scale %>%
  mutate(
    mean      = smear_sf["log_mu"]    * targets$log_mu$inv(log_mu),
    sd        = smear_sf["log_sigma"] * targets$log_sigma$inv(log_sigma),
    skewness  = smear_sf["log_gamma"] * targets$log_gamma$inv(log_gamma),
    kurtosis  = smear_sf["log_kappa"] * targets$log_kappa$inv(log_kappa)
  ) %>%
  select(mean, sd, skewness, kurtosis)

# --- Bind predictors to predictions ---
if(nrow(test_data_valid_predictors_rf) == nrow(pred_original_scale)) {
  pred_original_scale_with_predictors <- bind_cols(test_data_valid_predictors_rf, pred_original_scale)
} else {
  cat("ERROR: RF predictor and prediction row counts mismatch! Saving predictions only.\n")
  pred_original_scale_with_predictors <- pred_original_scale # Fallback
}

write_csv(pred_original_scale_with_predictors, "predictions_random_forest_upgraded.csv")


# --- Importance Tables ---
imp_long <- bind_rows(imps_long)
imp_avg  <- imp_long %>%
  group_by(feature) %>%
  summarise(average_importance = mean(importance, na.rm = TRUE), .groups = "drop") %>%
  arrange(desc(average_importance))

imp_wide <- imp_long %>%
  tidyr::pivot_wider(names_from = target, values_from = importance) %>%
  left_join(imp_avg, by = "feature") %>%
  arrange(desc(average_importance))

write_csv(imp_wide, "random_forest_feature_importances_upgraded.csv")


# --- Print both R^2 values for comparison ---
cat("\n--- Model Performance Comparison ---\n")

cat("\nIn-Sample R^2 (Performance on training data):\n")
print(round(in_sample_r2s, 3))

cat("\nOOB R^2 (Estimated performance on new data):\n")
print(round(oob_r2s, 3))

cat("\n(The gap between In-Sample and OOB R^2 is the amount of overfitting.)\n")

cat("\nSmearing factors (log targets only):\n"); print(round(smear_sf[smear_sf != 1], 3))
cat("\nWrote:\n - predictions_random_forest_upgraded.csv (now with predictors)\n - random_forest_feature_importances_upgraded.csv\n")
```

Some overfitting but not a lot


# Comparing model predictions

```{r}
# Load libraries for plotting and data manipulation
library(readr)
library(dplyr)
library(ggplot2)

# --- 1. Load Prediction Data ---

gam_preds <- read_csv("predictions_gam_hybrid.csv")
rf_preds <- read_csv("predictions_random_forest_upgraded.csv")
  
# --- 2. Prepare Data for Plotting ---
  
# Select predictors from one file and make them factors
predictors <- gam_preds %>%
  select(St, Re, Fr) %>%
  mutate(
    Re_factor = factor(Re),
    Fr_factor = factor(Fr) # 'Fr' is better as a factor for plotting
    )

# Select and rename GAM predictions
gam_preds_only <- gam_preds %>%
  select(mean, sd, skewness, kurtosis) %>%
  rename_with(~ paste0(., "_gam"), everything())
  
  # Select and rename RF predictions
rf_preds_only <- rf_preds %>%
  select(mean, sd, skewness, kurtosis) %>%
  rename_with(~ paste0(., "_rf"), everything())
  
  # Bind them all together
combined_preds <- bind_cols(predictors, gam_preds_only, rf_preds_only)

# --- 3. Create Individual Scatter Plots ---
  
  # Helper to find plot limits to ensure the y=x line is visible
  get_limits <- function(a, b) {
    all_vals <- c(a, b)
    range(all_vals, na.rm = TRUE)
  }

  # Plot 1: Mean (mu)
  mean_limits <- get_limits(combined_preds$mean_gam, combined_preds$mean_rf)
  p_mean <- ggplot(combined_preds, aes(x = mean_gam, y = mean_rf, color = Re_factor)) +
    geom_point(alpha = 0.8, size = 3) +
    geom_abline(intercept = 0, slope = 1, color = "red", linetype = "dashed") +
    labs(title = "Mean (mu) Agreement", 
         x = "GAM Prediction", y = "RF Prediction",
         color = "Re Factor") +
    xlim(mean_limits) + ylim(mean_limits) +
    theme_minimal()

  # Plot 2: Standard Deviation (sd)
  sd_limits <- get_limits(combined_preds$sd_gam, combined_preds$sd_rf)
  p_sd <- ggplot(combined_preds, aes(x = sd_gam, y = sd_rf, color = Re_factor)) +
    geom_point(alpha = 0.8, size = 3) +
    geom_abline(intercept = 0, slope = 1, color = "red", linetype = "dashed") +
    labs(title = "Standard Deviation (sigma) Agreement", 
         x = "GAM Prediction", y = "RF Prediction",
         color = "Re Factor") +
    xlim(sd_limits) + ylim(sd_limits) +
    theme_minimal()

  # Plot 3: Skewness (gamma)
  skew_limits <- get_limits(combined_preds$skewness_gam, combined_preds$skewness_rf)
  p_skew <- ggplot(combined_preds, aes(x = skewness_gam, y = skewness_rf, color = Re_factor)) +
    geom_point(alpha = 0.8, size = 3) +
    geom_abline(intercept = 0, slope = 1, color = "red", linetype = "dashed") +
    labs(title = "Skewness (gamma) Agreement", 
        x = "GAM Prediction", y = "RF Prediction",
        color = "Re Factor") +
    xlim(skew_limits) + ylim(skew_limits) +
    theme_minimal()

  # Plot 4: Kurtosis (kappa)
  kurt_limits <- get_limits(combined_preds$kurtosis_gam, combined_preds$kurtosis_rf)
  p_kurt <- ggplot(combined_preds, aes(x = kurtosis_gam, y = kurtosis_rf, color = Re_factor)) +
    geom_point(alpha = 0.8, size = 3) +
    geom_abline(intercept = 0, slope = 1, color = "red", linetype = "dashed") +
    labs(title = "Kurtosis (kappa) Agreement", 
          x = "GAM Prediction", y = "RF Prediction",
          color = "Re Factor") +
    xlim(kurt_limits) + ylim(kurt_limits) +
    theme_minimal()

  # --- 4. Combine and Display Plots ---
  (p_mean + p_sd) / (p_skew + p_kurt) +
    plot_annotation(
      title = "Model Comparison: GAM vs. Random Forest Predictions",
      subtitle = "Points colored by Reynolds Number (Re)",
      theme = theme(
        plot.title = element_text(size = 18, face = "bold", hjust = 0.5),
        plot.subtitle = element_text(size = 14, hjust = 0.5)
        )
    )
```

1. Overall Model Agreement is Excellent

The most important takeaway is that your two very different modeling approaches (a semi-parametric GAM and a non-parametric Random Forest) produced highly consistent results. This is fantastic news. It strongly suggests that our relationships that we have've modeled are real, robust features of the data, not just an artifact of one specific algorithm. When different methods converge on the same answer, it builds significant trust in the results.

2. Detailed Breakdown by Variable

a) Mean (mu): The points form three distinct diagonal clusters, each corresponding to one of the Reynolds numbers (Re_factor). The fact that all three clusters are so tight and fall directly on the red "Agreement (y=x)" line is a fantastic result. It visually confirms that the GAM model (log_mu ~ Re_factor + s(log_St_s)) is capturing the same complex relationships as the Random Forest, and both models are in complete consensus about the data's underlying patterns. There was no need to overcomplicate GAM. 

b) Standard Deviation (sigma): The agreement is near-perfect. For Standard Deviation (sigma), the points fall almost exactly on the red dashed line. This "y=x" line represents perfect agreement. This tells you that both models learned the exact same, strong patterns for predicting the standard deviation.

c) Skewness (gamma) & Kurtosis (kappa): For these higher-order moments, the agreement is still very good, but you can see more scatter. The points still follow the red line (low predictions from one model match low from the other), but with more variance. This is completely expected. These properties are inherently more "noisy" and difficult to model.

d) The Main Point of Divergence: Kurtosis: The Kurtosis (kappa) plot shows the most disagreement. While the models agree on the trend, the points are more scattered. The statistical analysis of the difference between the models (GAM - RF) confirms this. The GAM model systematically predicts slightly higher kurtosis values than the Random Forest. This isn't a sign that one model is "wrong," but rather that they have different ways of interpreting the complex interactions that lead to extreme kurtosis values. This is a valuable insight, as it isolates the single most complex metric that is hardest to predict.


In addition, needed to be mentioned is that as we can see for Re factor of 398 the models seem to have different opinions and disagree the most. This could be because we have the least amount of data for Re factor 398. 

## Displaying predictions

```{r}
# The prediction files now contain the predictors
gam_table <- read_csv("predictions_gam_hybrid.csv")
rf_table <- read_csv("predictions_random_forest_upgraded.csv")

# --- 2. Create GAM Table ---
cat("--- GAM Predictions with Test Data Predictors ---\n")
kable(gam_table, format = "pipe", digits = 4)


# --- 3. Create RF Table ---
cat("\n\n--- Random Forest Predictions with Test Data Predictors ---\n")
kable(rf_table, format = "pipe", digits = 4)
```
In this analysis, we applied a consistent Duan smearing factor to correct for re-transformation bias in both the Random Forest and the hybrid GAM predictions. Since our models were trained to predict log-transformed targets (like log_sigma, log_gamma, and log_kappa), simply exponentiating the final prediction would result in a systematically biased, low estimate. To correct this, we first calculated the residuals for each model on the log scale (e.g., log_sigma_actual - log_sigma_predicted) using the training data. We then computed a specific smearing factor for each target by taking the mean(exp(residuals)). This factor was then multiplied against the exponentiated test-set predictions (e.g., sd = smear_sigma * exp(log_sigma_pred)) to produce a final, unbiased prediction on the original scale, while log_mu was left un-smeared as a justified modeling choice.
